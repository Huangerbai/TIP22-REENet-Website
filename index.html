<!DOCTYPE HTML>
<html class="no-js">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <title>REENet</title>
    <meta name="description" content="Lithium Description" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <link href="css/plugins.css" media="screen" rel="stylesheet" type="text/css" />
    <link href="css/application.css" media="screen" rel="stylesheet" type="text/css" />
  </head>

<body>

    <!-- ABOUT -->

    <section id="page-about" class="section">
      <div align="center" style="padding-bottom: 100px;">
        <p class="copy-02">TIP 2022</p>
        <p class="heading h-01">Towards Low Light Enhancement with RAW Images</p>

        <p class="copy-02">
          <a href="https://Huangerbai.github.io/">Haofeng Huang</a> &nbsp;&nbsp;&nbsp;
          <a href="https://flyywh.github.io/">Wenhan Yang</a> &nbsp;&nbsp;&nbsp;
          <a href="https://huzi96.github.io/">Yueyu Hu</a> &nbsp;&nbsp;&nbsp;
          <a href="https://www.icst.pku.edu.cn/struct/people/liujiaying.html">Jiaying Liu</a> &nbsp;&nbsp;&nbsp;
          <a href="https://eecs.pku.edu.cn/info/1459/9475.htm">Ling-Yu Duan</a> &nbsp;&nbsp;&nbsp;
        </p>
      </div>

      <div class="site-inner">
        <h3 class="heading h-03">Abstract</h3>
            <p class="copy-02">In this paper, we make the first benchmark effort to elaborate on the superiority of using RAW images in the low light enhancement and develop a novel alternative route to utilize RAW images in a more flexible and practical way. Inspired by a full consideration on the typical image processing pipeline, we are inspired to develop a new evaluation framework, Factorized Enhancement Model (FEM), which decomposes the properties of RAW images into measurable factors and provides a tool for exploring how properties of RAW images affect the enhancement performance empirically. The empirical benchmark results show that the Linearity of data and Exposure Time recorded in meta-data play the most critical role, which brings distinct performance gains in various measures over the approaches taking the sRGB images as input. With the insights obtained from the benchmark results in mind, a RAW-guiding Exposure Enhancement Network (REENet) is developed, which makes trade-offs between the advantages and inaccessibility of RAW images in real applications in a way of using RAW images only in the training phase. REENet projects sRGB images into linear RAW domains to apply constraints with corresponding RAW images to reduce the difficulty of modeling training. After that, in the testing phase, our REENet does not rely on RAW images. Experimental results demonstrate not only the superiority of REENet to state-of-the-art sRGB-based methods and but also the effectiveness of the RAW guidance and all components.</p>
      </div>

      <div class="site-inner" style="padding-top: 50px;">
        <h3 class="heading h-03">Benchmark</h3>
        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="benchmark.jpg" width=90%> <br>
        </div>

        <p class='copy-02'>Factorized Enhancement Model (FEM): comparison of different ways to utilize RAW data for low light enhancement. FEM decomposes the properties of RAW images into measurable factors and provides a tool for exploring how properties of RAW images affect the enhancement performance empirically. 
        </p>

        <br><br>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="bench_result.jpg" width=70%> <br>
        </div>

        <p class='copy-02'>RAW Benchmarking: evaluation of sRGB/RAW based methods on SID dataset[1]. We provide subjective and objective comparison results using PSNR, SSIM, VIF[2], NIQE[3] and LPIPS[4].
        </p>
      </div>

      <div class="site-inner" style="padding-top: 50px;">
        <h3 class="heading h-03">REENet</h3>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="REENet.jpg" width=60%> <br>
        </div>

        <div align="center">
          <p class='copy-02'>RAW-guiding Exposure Enhancement Network (REENet): REENet makes full use of low/normal light RAW images in the training. Functionally, the three parts of the REENet perform unprocessing, enhancement and processing, respectively.
          </p>
        </div>

        <br><br>

        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="comp1.jpg" width=90%> <br>
        </div>

        <p class='copy-02'>Quantitative and qualitative comparison with traditional low light enhancement methods.
          
        <div align="center" style="padding-top:20px;padding-bottom:10px">
          <img src="comp2_1.jpg" width=90%> <br>
          <img src="comp2_2.jpg" width=90%> <br>
        </div>

        <p class='copy-02'>Quantitative and qualitative comparison with learning-based low light enhancement methods.
          
          

      </div>



      <div class="site-inner" style="padding-top:50px;">
        <p class="heading h-03"> Resources </p> 
          <ul style="line-height:1.5; padding-left: 50px; padding-right: 50px">
          　　<li class="copy-02"> Paper: <a href="https://arxiv.org/abs/2112.14022">arXiv</a></li>
          　　<li class="copy-02"> Code: Coming soon</li>
          </ul>
      </div>
      

      <div class="site-inner" style="padding-top:50px;">
        <p class='heading h-03'> Citation</p>
        <p class="copy-02"> @ARTICLE{Huang_TIP_2022, <br>
        &nbsp; &nbsp; author = {Huang, Haofeng and Yang, Wenhan and Hu, Yueyu and Liu, Jiaying and Duan, Ling-Yu}, <br>
        &nbsp; &nbsp; title = {Towards Low Light Enhancement With RAW Images}, <br>
        &nbsp; &nbsp; journal={IEEE Transactions on Image Processing}, <br>
        &nbsp; &nbsp; volume={31}, <br>
        &nbsp; &nbsp; pages={1391-1405}, <br>
        &nbsp; &nbsp; year={2022}, <br>
        } <br> 
        </p>
      </div>
      
      <div class="site-inner" style="padding-top:50px;">
        <p class="heading h-03"> Resources </p> 
          <p class="copy-02"> [1] C. Chen, Q. Chen, J. Xu, and V. Koltun, “Learning to see in the dark,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 3291–3300.</p>
          <br>
          <p class="copy-02"> [2] H. R. Sheikh and A. C. Bovik, “Image information and visual quality,” IEEE Trans. Image Process., vol. 15, no. 2, pp. 430–444, Feb. 2006.</p>
          <br>
          <p class="copy-02"> [3] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a 'completely blind' image quality analyzer,” IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209–212, Mar. 2013.</p>
          <br>
          <p class="copy-02"> [4] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable effectiveness of deep features as a perceptual metric,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 586–595.</p>
      </div>


    <section id="page-about" class="section">

</body>
</html>
